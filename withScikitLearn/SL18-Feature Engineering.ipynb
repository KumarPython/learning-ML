{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SL18-Feature Engineering",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGEc1gAjX88r"
      },
      "source": [
        "#**Feature Engineering**\r\n",
        "\r\n",
        "Feature engineering is a process of transforming the given data into a form which is easier to interpret. Here, we are interested in making it more transparent for a machine learning model, but some features can be generated so that the data visualization prepared for people without a data-related background can be more digestible. However, the concept of transparency for the machine learning models is a complicated thing as different models often require different approaches for the different kinds of data.\r\n",
        "\r\n",
        "* Imputation\r\n",
        "* Handling Outliers\r\n",
        "* Binning\r\n",
        "* Log Transform\r\n",
        "* One-Hot Encoding \r\n",
        "* Grouping Operations\r\n",
        "* Feature Split\r\n",
        "* Scaling\r\n",
        "* Extracting Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMxwsRBKYUYw"
      },
      "source": [
        "##**1-Missing Values -> Imputation**\r\n",
        "\r\n",
        "Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\r\n",
        "\r\n",
        "***Numerical Imputation***\r\n",
        "\r\n",
        "Imputation is a more preferable option rather than dropping because it preserves the data size. However, there is an important selection of what you impute to the missing values. I suggest beginning with considering a possible default value of missing values in the column. For example, if you have a column that only has 1 and NA, then it is likely that the NA rows correspond to 0. For another example, if you have a column that shows the “customer visit count in last month”, the missing values might be replaced with 0 as long as you think it is a sensible solution. \r\n",
        "\r\n",
        "Except for the case of having a default value for missing values, I think the best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect.\r\n",
        "\r\n",
        "***Categorical Imputation***\r\n",
        "\r\n",
        "Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection.\r\n",
        "\r\n",
        " ***An Extension To Imputation***\r\n",
        "\r\n",
        "Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.\r\n",
        "In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ztBYlDmYkhP"
      },
      "source": [
        "# NUMERICAL NAN\r\n",
        "\r\n",
        "#Filling all missing values with 0\r\n",
        "data = data.fillna(0)\r\n",
        "\r\n",
        "#Filling missing values with medians of the columns\r\n",
        "data = data.fillna(data.median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFV01LGjaOp3"
      },
      "source": [
        "# CATEGORICAL NAN\r\n",
        "\r\n",
        "#Max fill function for categorical columns\r\n",
        "data['column_name'].fillna(data['column_name'].value_counts()\r\n",
        ".idxmax(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz3usWLxkaqU"
      },
      "source": [
        "SimpleImputer(strategy='most_frequent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA1yuqAO0OxF",
        "outputId": "5906339a-eb72-4611-f46a-ffa4f4d766e3"
      },
      "source": [
        "# An Extension To Imputation\r\n",
        "import numpy as np\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean',add_indicator=True)\r\n",
        "imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\r\n",
        "SimpleImputer()\r\n",
        "X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\r\n",
        "print(imp_mean.transform(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7.   2.   3.   0. ]\n",
            " [ 4.   3.5  6.   1. ]\n",
            " [10.   3.5  9.   1. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8FhDgiMbe9a"
      },
      "source": [
        "##**2-Outliers**\r\n",
        "\r\n",
        "Before mentioning how outliers can be handled, I want to state that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision. Anyway, I am planning to focus visualization deeply in another article and let’s continue with statistical methodologies.\r\n",
        "\r\n",
        "Statistical methodologies are less precise as I mentioned, but on the other hand, they have a superiority, they are fast. Here I will list two different ways of handling outliers. These will detect them using standard deviation, and percentiles.\r\n",
        "\r\n",
        "***Outlier Detection with Standard Deviation***\r\n",
        "\r\n",
        "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\r\n",
        "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical.\r\n",
        "\r\n",
        "In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation.\r\n",
        "\r\n",
        "***Outlier Detection with Percentiles***\r\n",
        "\r\n",
        "Another mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier. The key point is here to set the percentage value once again, and this depends on the distribution of your data as mentioned earlier.\r\n",
        "Additionally, a common mistake is using the percentiles according to the range of the data. In other words, if your data ranges from 0 to 100, your top 5% is not the values between 96 and 100. Top 5% means here the values that are out of the 95th percentile of data.\r\n",
        "\r\n",
        "***An Outlier Dilemma: Drop or Cap***\r\n",
        "\r\n",
        "Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.\r\n",
        "On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Qmoms4dVMu"
      },
      "source": [
        "#Dropping the outlier rows with standard deviation\r\n",
        "factor = 3\r\n",
        "upper_lim = data['column'].mean () + data['column'].std () * factor\r\n",
        "lower_lim = data['column'].mean () - data['column'].std () * factor\r\n",
        "\r\n",
        "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOrkX811diDe"
      },
      "source": [
        "#Dropping the outlier rows with Percentiles\r\n",
        "upper_lim = data['column'].quantile(.95)\r\n",
        "lower_lim = data['column'].quantile(.05)\r\n",
        "\r\n",
        "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2-_MDX_duW2"
      },
      "source": [
        "#Capping the outlier rows with Percentiles\r\n",
        "upper_lim = data['column'].quantile(.95)\r\n",
        "lower_lim = data['column'].quantile(.05)\r\n",
        "data.loc[(df[column] > upper_lim),column] = upper_lim\r\n",
        "data.loc[(df[column] < lower_lim),column] = lower_lim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfdtEzjUfR1G"
      },
      "source": [
        "##**3-Smoothing/Binning**\r\n",
        "\r\n",
        "Binning method is used to smoothing data or to handle noisy data. In this method, the data is first sorted and then the sorted values are distributed into a number of buckets or bins. As binning methods consult the neighborhood of values, they perform local smoothing.\r\n",
        "* Sort the array of given data set.\r\n",
        "* Divides the range into N intervals, each containing the approximately same number of samples(Equal-depth partitioning).\r\n",
        "* Store mean/ median/ boundaries in each row.\r\n",
        "\r\n",
        "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized. (Please see regularization in machine learning)\r\n",
        "The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\r\n",
        "However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQTiwUD0gUaP"
      },
      "source": [
        "#Numerical Binning Example\r\n",
        "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVlZLyNIgf4G"
      },
      "source": [
        "#Categorical Binning Example\r\n",
        "conditions = [\r\n",
        "data['Country'].str.contains('Spain'),\r\n",
        "data['Country'].str.contains('Italy'),\r\n",
        "data['Country'].str.contains('Chile'),\r\n",
        "data['Country'].str.contains('Brazil')]\r\n",
        "\r\n",
        "choices = ['Europe', 'Europe', 'South America', 'South America']\r\n",
        "\r\n",
        "data['Continent'] = np.select(conditions, choices, default='Other')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTLFNVUPhsIj"
      },
      "source": [
        "##**4-Log Transformation**\r\n",
        "\r\n",
        "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. \r\n",
        "\r\n",
        "***What are the benefits of log transform:***\r\n",
        "\r\n",
        "* It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\r\n",
        "* In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.\r\n",
        "* It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.\r\n",
        "\r\n",
        "***A critical note:*** The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.\r\n",
        "\r\n",
        "Log(x+1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBsz8Ft9jE4D"
      },
      "source": [
        "#Log Transform Example\r\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\r\n",
        "data['log+1'] = (data['value']+1).transform(np.log)\r\n",
        "\r\n",
        "#Negative Values Handling\r\n",
        "#Note that the values are different\r\n",
        "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcT2WtSgjepa"
      },
      "source": [
        "##**5-Categorical Features**\r\n",
        "\r\n",
        "A categorical variable takes only a limited number of values.\r\n",
        "\r\n",
        "Three Approaches\r\n",
        "\r\n",
        "* ***Drop Categorical Variables***\r\n",
        "\r\n",
        "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\r\n",
        "\r\n",
        "* ***Label Encoding***\r\n",
        "\r\n",
        "Label encoding assigns each unique value to a different integer.\r\n",
        "This approach assumes an ordering of the categories.\r\n",
        "Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.\r\n",
        "\r\n",
        "* ***One-Hot Encoding***\r\n",
        "\r\n",
        "One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\r\n",
        "\r\n",
        "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\r\n",
        "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information.\r\n",
        "\r\n",
        "***Why One-Hot?:*** If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0mnsByykIO3"
      },
      "source": [
        "encoded_columns = pd.get_dummies(data['column'])\r\n",
        "data = data.join(encoded_columns).drop('column', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-tZGXSY7v8z"
      },
      "source": [
        "# Label Encoding\r\n",
        "\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "# Apply label encoder to each column with categorical data\r\n",
        "label_encoder = LabelEncoder()\r\n",
        "for col in object_cols:\r\n",
        "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\r\n",
        "    label_X_valid[col] = label_encoder.transform(X_valid[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICR_R_Zp8Q8D"
      },
      "source": [
        "# One-Hot Encoding\r\n",
        "\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "# Apply one-hot encoder to each column with categorical data\r\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\r\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayj_ZoBgk7At"
      },
      "source": [
        "##**6-Gouping**\r\n",
        "\r\n",
        "The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.\r\n",
        "\r\n",
        "***Categorical Column Grouping***\r\n",
        "\r\n",
        "I suggest three different ways for aggregating categorical columns:\r\n",
        "* The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\r\n",
        "* Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.\r\n",
        "* Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. \r\n",
        "\r\n",
        "***Numerical Column Grouping***\r\n",
        "\r\n",
        "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jp_awsDmdVE"
      },
      "source": [
        "# Grouping Based on Highest Frequency\r\n",
        "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Dpm_XGmrCp"
      },
      "source": [
        "#Pivot table Pandas Example\r\n",
        "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzvWOxb8npAQ"
      },
      "source": [
        "# Numerical Column Grouping\r\n",
        "\r\n",
        "#sum_cols: List of columns to sum\r\n",
        "#mean_cols: List of columns to average\r\n",
        "grouped = data.groupby('column_to_group')\r\n",
        "\r\n",
        "sums = grouped[sum_cols].sum().add_suffix('_sum')\r\n",
        "avgs = grouped[mean_cols].mean().add_suffix('_avg')\r\n",
        "\r\n",
        "new_df = pd.concat([sums, avgs], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAeK7lUboMsP"
      },
      "source": [
        "##**7-Split**\r\n",
        "\r\n",
        "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\r\n",
        "* We enable machine learning algorithms to comprehend them.\r\n",
        "* Make possible to bin and group them.\r\n",
        "* Improve model performance by uncovering potential information.\r\n",
        "\r\n",
        "Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ko1DHmkpRO7"
      },
      "source": [
        "data.name\r\n",
        "\r\n",
        "#Extracting first names\r\n",
        "data.name.str.split(\" \").map(lambda x: x[0])\r\n",
        "\r\n",
        "#Extracting last names\r\n",
        "data.name.str.split(\" \").map(lambda x: x[-1])\r\n",
        "\r\n",
        "#String extraction example\r\n",
        "data.title.head()\r\n",
        "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0lqXkpSp2tp"
      },
      "source": [
        "##**8-Scaling**\r\n",
        "\r\n",
        "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?\r\n",
        "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.\r\n",
        "\r\n",
        "Basically, there are two common ways of scaling:\r\n",
        "\r\n",
        "***Normalization***\r\n",
        "\r\n",
        "Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.\r\n",
        "\r\n",
        "***Standardization***\r\n",
        "\r\n",
        "Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.\r\n",
        "In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "D3naf_pQrIxz",
        "outputId": "1d9d79d1-d82c-4616-aa8d-0543f366ed5b"
      },
      "source": [
        "# Normalization\r\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\r\n",
        "\r\n",
        "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fbbae672ccab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mheubsPrqYT"
      },
      "source": [
        "# Standardization\r\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\r\n",
        "\r\n",
        "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhFtsJAutdZm"
      },
      "source": [
        "##**9-Dates**\r\n",
        "\r\n",
        "Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like \"01–01–2017\".\r\n",
        "Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. \r\n",
        "Here, I suggest three types of preprocessing for dates:\r\n",
        "\r\n",
        "* Extracting the parts of the date into different columns: Year, month, day, etc.\r\n",
        "* Extracting the time period between the current date and columns in terms of years, months, days, etc.\r\n",
        "*Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\r\n",
        "\r\n",
        "If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYb9eGSouuYh",
        "outputId": "62ebaf9d-81db-42c9-90bb-47f421a43271"
      },
      "source": [
        "import pandas as pd\r\n",
        "from datetime import date\r\n",
        "\r\n",
        "data = pd.DataFrame({'date':\r\n",
        "['01-01-2017',\r\n",
        "'04-12-2008',\r\n",
        "'23-06-1988',\r\n",
        "'25-08-1999',\r\n",
        "'20-02-1993',\r\n",
        "]})\r\n",
        "\r\n",
        "#Transform string to date\r\n",
        "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\r\n",
        "\r\n",
        "#Extracting Year\r\n",
        "data['year'] = data['date'].dt.year\r\n",
        "\r\n",
        "#Extracting Month\r\n",
        "data['month'] = data['date'].dt.month\r\n",
        "\r\n",
        "#Extracting passed years since the date\r\n",
        "data['passed_years'] = date.today().year - data['date'].dt.year\r\n",
        "\r\n",
        "#Extracting passed months since the date\r\n",
        "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\r\n",
        "\r\n",
        "#Extracting the weekday name of the date\r\n",
        "data['day_name'] = data['date'].dt.day_name()\r\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>passed_years</th>\n",
              "      <th>passed_months</th>\n",
              "      <th>day_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>48</td>\n",
              "      <td>Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-12-04</td>\n",
              "      <td>2008</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>145</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1988-06-23</td>\n",
              "      <td>1988</td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "      <td>391</td>\n",
              "      <td>Thursday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1999-08-25</td>\n",
              "      <td>1999</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "      <td>257</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1993-02-20</td>\n",
              "      <td>1993</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>335</td>\n",
              "      <td>Saturday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  year  month  passed_years  passed_months   day_name\n",
              "0 2017-01-01  2017      1             4             48     Sunday\n",
              "1 2008-12-04  2008     12            13            145   Thursday\n",
              "2 1988-06-23  1988      6            33            391   Thursday\n",
              "3 1999-08-25  1999      8            22            257  Wednesday\n",
              "4 1993-02-20  1993      2            28            335   Saturday"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Cu2U8AKFFj"
      },
      "source": [
        "##**10-Feature Selection**\r\n",
        "\r\n",
        "Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\r\n",
        "\r\n",
        "***Irrelevant or partially relevant features can negatively impact model performance.Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.***\r\n",
        "\r\n",
        "Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\r\n",
        "\r\n",
        "***How to select features and what are Benefits of performing feature selection before modeling your data?***\r\n",
        "\r\n",
        "* **Reduces Overfitting:** Less redundant data means less opportunity to make decisions based on noise.\r\n",
        "* **Improves Accuracy:** Less misleading data means modeling accuracy improves.\r\n",
        "* **Reduces Training Time:** fewer data points reduce algorithm complexity and algorithms train faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7InXAvvDP7a8",
        "outputId": "79c94ca0-de68-4d2b-b430-913b6dd4fba4"
      },
      "source": [
        "# Using selectKbest\r\n",
        "\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "from sklearn.feature_selection import SelectKBest, chi2\r\n",
        "X, y = load_digits(return_X_y=True)\r\n",
        "X.shape\r\n",
        "\r\n",
        "X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\r\n",
        "X_new.shape\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzzAC5jGRTiH",
        "outputId": "8ab26296-cc84-4d59-af74-d73185b8dac6"
      },
      "source": [
        "# Using SelectfromModel\r\n",
        "\r\n",
        "from sklearn.linear_model import Lasso\r\n",
        "from sklearn.feature_selection import SelectFromModel\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "X, y = load_digits(return_X_y=True)\r\n",
        "\r\n",
        "### Apply Feature Selection\r\n",
        "# first, I specify the Lasso Regression model, and I select a suitable alpha (equivalent of penalty). The bigger the \r\n",
        "# alpha the less features that will be selected. Then I use the selectFromModel object from sklearn, which\r\n",
        "# will select the features which coefficients are non-zero\r\n",
        "\r\n",
        "model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\r\n",
        "model.fit(X, y)\r\n",
        "model.get_support()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
              "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
              "        True,  True,  True, False, False,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE2zHE-f_YOT"
      },
      "source": [
        "##**11-Imbalanced DataSet**\r\n",
        "\r\n",
        "Imbalanced datasets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class\r\n",
        "\r\n",
        "Imbalanced datasets can be found for different use cases in various domains:\r\n",
        "\r\n",
        "* **Finance:** Fraud detection datasets commonly have a fraud rate of ~1–2%\r\n",
        "* **Ad Serving:** Click prediction datasets also don’t have a high clickthrough rate.\r\n",
        "* **Transportation/Airline:** Will Airplane failure occur?\r\n",
        "* **Medical:** Does a patient has cancer?\r\n",
        "* **Content moderation:** Does a post contain NSFW content?\r\n",
        "\r\n",
        "![imb.png](data:image/png;base64,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)\r\n",
        "\r\n",
        "***Undersampling using Tomek Links:***\r\n",
        " \r\n",
        "One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.\r\n",
        "\r\n",
        "In this algorithm, we end up removing the majority element from the Tomek link, which provides a better decision boundary for a classifier.\r\n",
        "\r\n",
        "***Oversampling using SMOTE:***\r\n",
        " \r\n",
        "In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.\r\n",
        "\r\n",
        "\r\n",
        "* Random resampling provides a naive technique for rebalancing the class distribution for an imbalanced dataset.\r\n",
        "* Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\r\n",
        "* Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model.\r\n",
        "\r\n",
        "***Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.***\r\n",
        "\r\n",
        "Importantly, the change to the class distribution is only applied to the training dataset. The intent is to influence the fit of the models. The resampling is not applied to the test or holdout dataset used to evaluate the performance of a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryu9pHZ2AkE4"
      },
      "source": [
        "# UnderSampling\r\n",
        "\r\n",
        "from imblearn.under_sampling import TomekLinks\r\n",
        "tl = TomekLinks(return_indices=True, ratio='majority')\r\n",
        "X_tl, y_tl, id_tl = tl.fit_sample(X, y)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renYVIGaG2FA"
      },
      "source": [
        "# DownSampling\r\n",
        "\r\n",
        "from imblearn.under_sampling import NearMiss\r\n",
        "\r\n",
        "# Implementing Undersampling for Handling Imbalanced \r\n",
        "nm = NearMiss(random_state=42)\r\n",
        "X_res,y_res=nm.fit_sample(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p0dmnbPHwq_"
      },
      "source": [
        "# OverSampling\r\n",
        "# INCREASE SAMPLE\r\n",
        "\r\n",
        "from imblearn.combine import SMOTETomek\r\n",
        "# Implementing Oversampling for Handling Imbalanced \r\n",
        "smk = SMOTETomek(random_state=42)\r\n",
        "X_res,y_res=smk.fit_sample(X,Y)\r\n",
        "# OR\r\n",
        "## RandomOverSampler to handle imbalanced data\r\n",
        "\r\n",
        "from imblearn.over_sampling import RandomOverSampler\r\n",
        "os =  RandomOverSampler(ratio=0.5)\r\n",
        "# This would ensure that the minority class was oversampled to have half the number of examples as the majority \r\n",
        "# class, for binary classification problems. This means that if the majority class had 1,000 examples and the \r\n",
        "# minority class had 100, the transformed dataset would have 500 examples of the minority class.\r\n",
        "X_train_res, y_train_res = os.fit_sample(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}